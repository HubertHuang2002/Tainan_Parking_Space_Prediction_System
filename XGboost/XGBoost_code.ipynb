{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3714b2af",
   "metadata": {},
   "source": [
    "Selection Top 20 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308589f7",
   "metadata": {
    "id": "308589f7",
    "outputId": "fce3867b-4aa9-4ac8-b3ba-c8ebaeb185c7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# load data\n",
    "df1 = pd.read_csv(\"../區域資料_終版/中西_dataset.csv\")\n",
    "df2 = pd.read_csv(\"../區域資料_終版/安平_dataset.csv\")\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# correct erroneous values\n",
    "df.loc[(df[\"ParkingSegmentID\"] == 1192) & (df[\"half_hour_interval\"] == 20) & (df[\"month_val\"] == 3) & (df[\"day_val\"] == 8), \"TotalSpaces\"] = 63\n",
    "df.drop(df[(df['ParkingSegmentID'] == 1131) & (df['year_val'] == 2025) & (df['month_val'] == 3) & (df['day_val'] == 27)].index, inplace=True)\n",
    "condition = df[\"avg_available_spots\"] > df[\"TotalSpaces\"]\n",
    "df.loc[condition, \"avg_available_spots\"] = df.loc[condition, \"TotalSpaces\"]\n",
    "\n",
    "# Add day_off and wind direction sin/cos\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df['day_off'] = df['datetime'].dt.weekday.isin([5, 6]) | df['is_national_holiday']\n",
    "df['day_off'] = df['day_off'].astype(int)\n",
    "\n",
    "# wind direction sin/cos transformation\n",
    "df['wind_direction_10m_sin'] = np.sin(np.deg2rad(df['wind_direction_10m']))\n",
    "df['wind_direction_10m_cos'] = np.cos(np.deg2rad(df['wind_direction_10m']))\n",
    "df['wind_direction_80m_sin'] = np.sin(np.deg2rad(df['wind_direction_80m']))\n",
    "df['wind_direction_80m_cos'] = np.cos(np.deg2rad(df['wind_direction_80m']))\n",
    "df['wind_direction_120m_sin'] = np.sin(np.deg2rad(df['wind_direction_120m']))\n",
    "df['wind_direction_120m_cos'] = np.cos(np.deg2rad(df['wind_direction_120m']))\n",
    "df.drop(columns=[\"wind_direction_10m\",\"wind_direction_120m\",\"wind_direction_80m\",\"holiday_name\",\"is_national_holiday\"], inplace=True)\n",
    "\n",
    "# time split\n",
    "df = df.dropna(subset=['avg_available_spots'])\n",
    "train_start = pd.to_datetime(\"2024-01-23\")\n",
    "train_end   = pd.to_datetime(\"2025-01-22\")\n",
    "valid_start = pd.to_datetime(\"2025-01-23\")\n",
    "valid_end   = pd.to_datetime(\"2025-03-22\")\n",
    "test_start  = pd.to_datetime(\"2025-03-23\")\n",
    "test_end    = pd.to_datetime(\"2025-05-09\")\n",
    "\n",
    "\n",
    "df_train = df[(df['datetime'] >= train_start) & (df['datetime'] <= train_end)].copy()\n",
    "df_valid = df[(df['datetime'] >= valid_start) & (df['datetime'] <= valid_end)].copy()\n",
    "df_test  = df[(df['datetime'] >= test_start) & (df['datetime'] <= test_end)].copy()\n",
    "\n",
    "# remove unnecessary columns\n",
    "drop_cols = ['datetime', 'datetime_hour', 'time', 'date', 'ParkingSegmentID']\n",
    "for dataset in [df_train, df_valid, df_test]:\n",
    "    for col in drop_cols:\n",
    "        if col in dataset.columns:\n",
    "            dataset.drop(columns=col, inplace=True)\n",
    "\n",
    "# create X and y, including cyclical features and one-hot encoding\n",
    "def prepare_xy(df_part):\n",
    "    y = df_part['avg_available_spots']\n",
    "    X = df_part.drop(columns=['avg_available_spots'])\n",
    "\n",
    "    X['sin_day'] = np.sin(2 * np.pi * X['day_val'] / 31)\n",
    "    X['cos_day'] = np.cos(2 * np.pi * X['day_val'] / 31)\n",
    "    X['sin_halfhour'] = np.sin(2 * np.pi * X['half_hour_interval'] / 48)\n",
    "    X['cos_halfhour'] = np.cos(2 * np.pi * X['half_hour_interval'] / 48)\n",
    "    X.drop(columns=['day_val', 'half_hour_interval'], inplace=True)\n",
    "\n",
    "    cat_cols = ['month_val', 'weekday_val'] + X.select_dtypes(include=['object']).columns.tolist()\n",
    "    cat_cols_exist = [col for col in cat_cols if col in X.columns]\n",
    "    X = pd.get_dummies(X, columns=cat_cols_exist, drop_first=False)\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = prepare_xy(df_train)\n",
    "X_valid, y_valid = prepare_xy(df_valid)\n",
    "X_test,  y_test  = prepare_xy(df_test)\n",
    "\n",
    "X_valid = X_valid.reindex(columns=X_train.columns, fill_value=0)\n",
    "X_test  = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "# grid search\n",
    "params = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [4, 6],\n",
    "    'learning_rate': [0.05, 0.1]\n",
    "}\n",
    "model = XGBRegressor(random_state=42)\n",
    "grid = GridSearchCV(model, param_grid=params, cv=3, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# evaluate the test set\n",
    "for name, X_part, y_part in [('\\u6e2c\\u8a66\\u96c6', X_test, y_test)]:\n",
    "    y_pred = best_model.predict(X_part)\n",
    "    print(f\"\\n\\ud83d\\udcca {name} \\u8a55\\u4f30：\")\n",
    "    print(f\"MAE: {mean_absolute_error(y_part, y_pred):.2f}\")\n",
    "    print(f\"RMSE: {mean_squared_error(y_part, y_pred)**0.5:.2f}\")\n",
    "    print(f\"R2 Score: {r2_score(y_part, y_pred):.4f}\")\n",
    "\n",
    "# feature importance analysis\n",
    "booster = best_model.get_booster()\n",
    "importance_types = ['gain', 'weight', 'cover']\n",
    "importance_df = pd.DataFrame({'feature': X_train.columns})\n",
    "\n",
    "for imp_type in importance_types:\n",
    "    scores = booster.get_score(importance_type=imp_type)\n",
    "    importance_df[imp_type] = importance_df['feature'].map(scores).fillna(0)\n",
    "\n",
    "importance_df_sorted = importance_df.sort_values(by='gain', ascending=False)\n",
    "\n",
    "print(\"\\n\\ud83d\\udccb Top 22 \\u7279\\u5fb5 (by gain):\")\n",
    "print(importance_df_sorted.head(20).to_string(index=False))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=importance_df_sorted.head(20), x='gain', y='feature')\n",
    "plt.title(\"Top 20 \\u7279\\u5fb5\\u91cd\\u8981\\u6027 (by Gain)\")\n",
    "plt.xlabel(\"Gain\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906d117b",
   "metadata": {},
   "source": [
    "Regression with All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfbdccf",
   "metadata": {
    "id": "4dfbdccf",
    "outputId": "7d60e24b-6cf1-4ffc-b421-424610f589f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "\n",
      "📊 [Test]\n",
      "MSE: 103.86\n",
      "Adjusted R²: 0.9720\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, r2_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "# ========= data loading and preprocessing =========\n",
    "df1 = pd.read_csv(\"../區域資料_終版/中西_dataset.csv\")\n",
    "df2 = pd.read_csv(\"../區域資料_終版/安平_dataset.csv\")\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# correct erroneous values\n",
    "df.loc[(df[\"ParkingSegmentID\"] == 1192) & (df[\"half_hour_interval\"] == 20) & (df[\"month_val\"] == 3) & (df[\"day_val\"] == 8), \"TotalSpaces\"] = 63\n",
    "df.drop(df[(df['ParkingSegmentID'] == 1131) & (df['year_val'] == 2025) & (df['month_val'] == 3) & (df['day_val'] == 27)].index, inplace=True)\n",
    "df.loc[df[\"avg_available_spots\"] > df[\"TotalSpaces\"], \"avg_available_spots\"] = df[\"TotalSpaces\"]\n",
    "\n",
    "# Add `day_off` and wind direction `sin`/`cos`.\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df['day_off'] = df['datetime'].dt.weekday.isin([5, 6]) | df['is_national_holiday']\n",
    "df['day_off'] = df['day_off'].astype(int)\n",
    "for height in [10, 80, 120]:\n",
    "    df[f'wind_direction_{height}m_sin'] = np.sin(np.deg2rad(df[f'wind_direction_{height}m']))\n",
    "    df[f'wind_direction_{height}m_cos'] = np.cos(np.deg2rad(df[f'wind_direction_{height}m']))\n",
    "df.drop(columns=[\n",
    "    \"wind_direction_10m\", \"wind_direction_80m\", \"wind_direction_120m\",\n",
    "    \"holiday_name\", \"is_national_holiday\"\n",
    "], inplace=True)\n",
    "\n",
    "# time split\n",
    "df = df.dropna(subset=['avg_available_spots'])\n",
    "train_start = pd.to_datetime(\"2024-01-23\")\n",
    "train_end = pd.to_datetime(\"2025-01-22\")\n",
    "valid_start = pd.to_datetime(\"2025-01-23\")\n",
    "valid_end = pd.to_datetime(\"2025-03-22\")\n",
    "test_start = pd.to_datetime(\"2025-03-23\")\n",
    "test_end = pd.to_datetime(\"2025-05-09\")\n",
    "\n",
    "df_train = df[(df['datetime'] >= train_start) & (df['datetime'] <= train_end)].copy()\n",
    "df_valid = df[(df['datetime'] >= valid_start) & (df['datetime'] <= valid_end)].copy()\n",
    "df_test = df[(df['datetime'] >= test_start) & (df['datetime'] <= test_end)].copy()\n",
    "\n",
    "drop_cols = ['datetime', 'datetime_hour', 'time', 'date', 'ParkingSegmentID']\n",
    "for dataset in [df_train, df_valid, df_test]:\n",
    "    for col in drop_cols:\n",
    "        if col in dataset.columns:\n",
    "            dataset.drop(columns=col, inplace=True)\n",
    "\n",
    "# ========= feature engineering function =========\n",
    "def prepare_xy(df_part):\n",
    "    y = df_part['avg_available_spots']\n",
    "    X = df_part.drop(columns=['avg_available_spots'])\n",
    "\n",
    "    # cyclical features\n",
    "    X['sin_day'] = np.sin(2 * np.pi * X['day_val'] / 31)\n",
    "    X['cos_day'] = np.cos(2 * np.pi * X['day_val'] / 31)\n",
    "    X['sin_halfhour'] = np.sin(2 * np.pi * X['half_hour_interval'] / 48)\n",
    "    X['cos_halfhour'] = np.cos(2 * np.pi * X['half_hour_interval'] / 48)\n",
    "    X.drop(columns=['day_val', 'half_hour_interval'], inplace=True)\n",
    "\n",
    "    # One-Hot encoding\n",
    "    cat_cols = ['month_val', 'weekday_val'] + X.select_dtypes(include=['object']).columns.tolist()\n",
    "    cat_cols_exist = [col for col in cat_cols if col in X.columns]\n",
    "    X = pd.get_dummies(X, columns=cat_cols_exist, drop_first=False)\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = prepare_xy(df_train)\n",
    "X_valid, y_valid = prepare_xy(df_valid)\n",
    "X_test, y_test = prepare_xy(df_test)\n",
    "\n",
    "# ensure column consistency\n",
    "X_valid = X_valid.reindex(columns=X_train.columns, fill_value=0)\n",
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "# ========= model training and frid search =========\n",
    "params = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [4, 6],\n",
    "    'learning_rate': [0.05, 0.1]\n",
    "}\n",
    "model = XGBRegressor(random_state=42)\n",
    "grid = GridSearchCV(model, param_grid=params, cv=3, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# ========= prediction and metrics =========\n",
    "def adjusted_r2(r2, n, k):\n",
    "    return 1 - (1 - r2) * ((n - 1) / (n - k - 1))\n",
    "\n",
    "def eval_all(X, y, dataset_name):\n",
    "    y_pred = best_model.predict(X)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    adj_r2 = adjusted_r2(r2, len(y), X.shape[1])\n",
    "\n",
    "    # classification task: whether there is an available space\n",
    "    y_true_bin = (y > 0).astype(int)\n",
    "    best_threshold = 0.5\n",
    "    best_f1 = 0\n",
    "    for t in np.arange(0.1, 0.91, 0.01):\n",
    "        pred_bin = (y_pred > t).astype(int)\n",
    "        f1 = f1_score(y_true_bin, pred_bin)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = t\n",
    "\n",
    "    pred_bin_best = (y_pred > best_threshold).astype(int)\n",
    "    acc = accuracy_score(y_true_bin, pred_bin_best)\n",
    "    prec = precision_score(y_true_bin, pred_bin_best)\n",
    "    rec = recall_score(y_true_bin, pred_bin_best)\n",
    "    f1 = f1_score(y_true_bin, pred_bin_best)\n",
    "    roc = roc_auc_score(y_true_bin, y_pred)\n",
    "    prc = average_precision_score(y_true_bin, y_pred)\n",
    "\n",
    "    print(f\"\\n📊 [{dataset_name}]\")\n",
    "    print(f\"MSE: {mse:.2f}\")\n",
    "    print(f\"Adjusted R²: {adj_r2:.4f}\")\n",
    "    # print(f\"R²: {r2:.4f}\")\n",
    "    # print(f\"Accuracy: {acc:.4f}\")\n",
    "    # print(f\"Precision: {prec:.4f}\")\n",
    "    # print(f\"Recall: {rec:.4f}\")\n",
    "    # print(f\"F1 Score: {f1:.4f}\")\n",
    "    # print(f\"ROC-AUC: {roc:.4f}\")\n",
    "    # print(f\"PRC-AUC: {prc:.4f}\")\n",
    "    # print(f\"Best Threshold: {best_threshold:.3f}\")\n",
    "\n",
    "# evalation\n",
    "# eval_all(X_train, y_train, \"Train\")\n",
    "# eval_all(X_valid, y_valid, \"Valid\")\n",
    "eval_all(X_test, y_test, \"Test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd871b8",
   "metadata": {},
   "source": [
    "Regression with Features of the Four Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22089493",
   "metadata": {
    "id": "22089493",
    "outputId": "8bec7426-e53b-4796-bbc7-f97c8c5325b1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r9/vznk0t21541cq588wv51gt6m0000gn/T/ipykernel_3636/147346167.py:19: DtypeWarning: Columns (47,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df1 = pd.read_csv(\"../區域資料_終版/中西_dataset.csv\")\n",
      "/var/folders/r9/vznk0t21541cq588wv51gt6m0000gn/T/ipykernel_3636/147346167.py:20: DtypeWarning: Columns (0,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df2 = pd.read_csv(\"../區域資料_終版/安平_dataset.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "\n",
      "📊 [Test - 投票四票]\n",
      "MSE: 129.21\n",
      "Adjusted R²: 0.9652\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# feature list\n",
    "selected_features_4_votes = [\n",
    "    'TotalSpaces', 'temperature_2m', 'hour_val', 'day_off',\n",
    "    'wind_speed_10m', 'wind_speed_80m', 'wind_speed_120m',\n",
    "    'wind_direction_10m_sin', 'wind_direction_10m_cos',\n",
    "    'wind_direction_80m_sin', 'wind_direction_80m_cos',\n",
    "    'wind_direction_120m_sin', 'wind_direction_120m_cos',\n",
    "    'weather_code', 'sin_halfhour', 'cos_halfhour',\n",
    "    'sin_day', 'cos_day'\n",
    "]\n",
    "\n",
    "# data loading and preprocessing\n",
    "df1 = pd.read_csv(\"../區域資料_終版/中西_dataset.csv\")\n",
    "df2 = pd.read_csv(\"../區域資料_終版/安平_dataset.csv\")\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# correct erroneous values\n",
    "df.loc[(df[\"ParkingSegmentID\"] == 1192) & (df[\"half_hour_interval\"] == 20) & (df[\"month_val\"] == 3) & (df[\"day_val\"] == 8), \"TotalSpaces\"] = 63\n",
    "df.drop(df[(df['ParkingSegmentID'] == 1131) & (df['year_val'] == 2025) & (df['month_val'] == 3) & (df['day_val'] == 27)].index, inplace=True)\n",
    "df.loc[df[\"avg_available_spots\"] > df[\"TotalSpaces\"], \"avg_available_spots\"] = df[\"TotalSpaces\"]\n",
    "\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df['day_off'] = df['datetime'].dt.weekday.isin([5, 6]) | df['is_national_holiday']\n",
    "df['day_off'] = df['day_off'].astype(int)\n",
    "for height in [10, 80, 120]:\n",
    "    df[f'wind_direction_{height}m_sin'] = np.sin(np.deg2rad(df[f'wind_direction_{height}m']))\n",
    "    df[f'wind_direction_{height}m_cos'] = np.cos(np.deg2rad(df[f'wind_direction_{height}m']))\n",
    "df.drop(columns=[\n",
    "    \"wind_direction_10m\", \"wind_direction_80m\", \"wind_direction_120m\",\n",
    "    \"holiday_name\", \"is_national_holiday\"\n",
    "], inplace=True)\n",
    "\n",
    "# time split\n",
    "df = df.dropna(subset=['avg_available_spots'])\n",
    "train_start = pd.to_datetime(\"2024-01-23\")\n",
    "train_end = pd.to_datetime(\"2025-01-22\")\n",
    "valid_start = pd.to_datetime(\"2025-01-23\")\n",
    "valid_end = pd.to_datetime(\"2025-03-22\")\n",
    "test_start = pd.to_datetime(\"2025-03-23\")\n",
    "test_end = pd.to_datetime(\"2025-05-09\")\n",
    "\n",
    "df_train = df[(df['datetime'] >= train_start) & (df['datetime'] <= train_end)].copy()\n",
    "df_valid = df[(df['datetime'] >= valid_start) & (df['datetime'] <= valid_end)].copy()\n",
    "df_test = df[(df['datetime'] >= test_start) & (df['datetime'] <= test_end)].copy()\n",
    "\n",
    "drop_cols = ['datetime', 'datetime_hour', 'time', 'date', 'ParkingSegmentID']\n",
    "for dataset in [df_train, df_valid, df_test]:\n",
    "    for col in drop_cols:\n",
    "        if col in dataset.columns:\n",
    "            dataset.drop(columns=col, inplace=True)\n",
    "\n",
    "# feature engineering + cyclical features + One-Hot encoding\n",
    "def prepare_xy_vote(df_part):\n",
    "    y = df_part['avg_available_spots']\n",
    "    X = df_part.drop(columns=['avg_available_spots'])\n",
    "\n",
    "    X['sin_day'] = np.sin(2 * np.pi * X['day_val'] / 31)\n",
    "    X['cos_day'] = np.cos(2 * np.pi * X['day_val'] / 31)\n",
    "    X['sin_halfhour'] = np.sin(2 * np.pi * X['half_hour_interval'] / 48)\n",
    "    X['cos_halfhour'] = np.cos(2 * np.pi * X['half_hour_interval'] / 48)\n",
    "    X.drop(columns=['day_val', 'half_hour_interval'], inplace=True)\n",
    "\n",
    "    cat_cols = ['month_val', 'weekday_val'] + X.select_dtypes(include=['object']).columns.tolist()\n",
    "    cat_cols_exist = [col for col in cat_cols if col in X.columns]\n",
    "    X = pd.get_dummies(X, columns=cat_cols_exist, drop_first=False)\n",
    "\n",
    "    X = X.reindex(columns=selected_features_4_votes, fill_value=0)\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = prepare_xy_vote(df_train)\n",
    "X_valid, y_valid = prepare_xy_vote(df_valid)\n",
    "X_test, y_test = prepare_xy_vote(df_test)\n",
    "\n",
    "# model training\n",
    "params = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [4, 6],\n",
    "    'learning_rate': [0.05, 0.1]\n",
    "}\n",
    "model = XGBRegressor(random_state=42)\n",
    "grid = GridSearchCV(model, param_grid=params, cv=3, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# prediction and evaluation\n",
    "def adjusted_r2(r2, n, k):\n",
    "    return 1 - (1 - r2) * ((n - 1) / (n - k - 1))\n",
    "\n",
    "def eval_regression(X, y, dataset_name):\n",
    "    y_pred = best_model.predict(X)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    adj_r2 = adjusted_r2(r2, len(y), X.shape[1])\n",
    "    print(f\"\\n📊 [{dataset_name} - 投票四票]\")\n",
    "    print(f\"MSE: {mse:.2f}\")\n",
    "    print(f\"Adjusted R²: {adj_r2:.4f}\")\n",
    "\n",
    "# evaluation\n",
    "# eval_regression(X_train, y_train, \"Train\")\n",
    "# eval_regression(X_valid, y_valid, \"Valid\")\n",
    "eval_regression(X_test, y_test, \"Test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a276069",
   "metadata": {},
   "source": [
    "Regression with Features of the Three Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1463c41",
   "metadata": {
    "id": "b1463c41",
    "outputId": "7cbc5246-e076-49a0-b462-04e475fff38e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r9/vznk0t21541cq588wv51gt6m0000gn/T/ipykernel_3636/1861241395.py:17: DtypeWarning: Columns (47,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df1 = pd.read_csv(\"../區域資料_終版/中西_dataset.csv\")\n",
      "/var/folders/r9/vznk0t21541cq588wv51gt6m0000gn/T/ipykernel_3636/1861241395.py:18: DtypeWarning: Columns (0,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df2 = pd.read_csv(\"../區域資料_終版/安平_dataset.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "\n",
      "📊 [Test - 投票三票]\n",
      "MSE: 128.53\n",
      "Adjusted R²: 0.9654\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# feature list\n",
    "selected_features_3_votes = [\n",
    "    'TotalSpaces', 'temperature_2m', 'hour_val', 'day_off',\n",
    "    'wind_speed_10m', 'wind_speed_80m',\n",
    "    'wind_direction_10m_sin', 'wind_direction_10m_cos',\n",
    "    'weather_code', 'sin_halfhour', 'cos_halfhour',\n",
    "    'sin_day', 'cos_day'\n",
    "]\n",
    "\n",
    "# data loading and preprocessing\n",
    "df1 = pd.read_csv(\"../區域資料_終版/中西_dataset.csv\")\n",
    "df2 = pd.read_csv(\"../區域資料_終版/安平_dataset.csv\")\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# correct erroneous values\n",
    "df.loc[(df[\"ParkingSegmentID\"] == 1192) & (df[\"half_hour_interval\"] == 20) & (df[\"month_val\"] == 3) & (df[\"day_val\"] == 8), \"TotalSpaces\"] = 63\n",
    "df.drop(df[(df['ParkingSegmentID'] == 1131) & (df['year_val'] == 2025) & (df['month_val'] == 3) & (df['day_val'] == 27)].index, inplace=True)\n",
    "df.loc[df[\"avg_available_spots\"] > df[\"TotalSpaces\"], \"avg_available_spots\"] = df[\"TotalSpaces\"]\n",
    "\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df['day_off'] = df['datetime'].dt.weekday.isin([5, 6]) | df['is_national_holiday']\n",
    "df['day_off'] = df['day_off'].astype(int)\n",
    "for height in [10, 80, 120]:\n",
    "    df[f'wind_direction_{height}m_sin'] = np.sin(np.deg2rad(df[f'wind_direction_{height}m']))\n",
    "    df[f'wind_direction_{height}m_cos'] = np.cos(np.deg2rad(df[f'wind_direction_{height}m']))\n",
    "df.drop(columns=[\n",
    "    \"wind_direction_10m\", \"wind_direction_80m\", \"wind_direction_120m\",\n",
    "    \"holiday_name\", \"is_national_holiday\"\n",
    "], inplace=True)\n",
    "\n",
    "# time split\n",
    "df = df.dropna(subset=['avg_available_spots'])\n",
    "train_start = pd.to_datetime(\"2024-01-23\")\n",
    "train_end = pd.to_datetime(\"2025-01-22\")\n",
    "valid_start = pd.to_datetime(\"2025-01-23\")\n",
    "valid_end = pd.to_datetime(\"2025-03-22\")\n",
    "test_start = pd.to_datetime(\"2025-03-23\")\n",
    "test_end = pd.to_datetime(\"2025-05-09\")\n",
    "\n",
    "df_train = df[(df['datetime'] >= train_start) & (df['datetime'] <= train_end)].copy()\n",
    "df_valid = df[(df['datetime'] >= valid_start) & (df['datetime'] <= valid_end)].copy()\n",
    "df_test = df[(df['datetime'] >= test_start) & (df['datetime'] <= test_end)].copy()\n",
    "\n",
    "drop_cols = ['datetime', 'datetime_hour', 'time', 'date', 'ParkingSegmentID']\n",
    "for dataset in [df_train, df_valid, df_test]:\n",
    "    for col in drop_cols:\n",
    "        if col in dataset.columns:\n",
    "            dataset.drop(columns=col, inplace=True)\n",
    "\n",
    "# feature engineering + cyclical features + One-Hot encoding\n",
    "def prepare_xy_vote(df_part):\n",
    "    y = df_part['avg_available_spots']\n",
    "    X = df_part.drop(columns=['avg_available_spots'])\n",
    "\n",
    "    X['sin_day'] = np.sin(2 * np.pi * X['day_val'] / 31)\n",
    "    X['cos_day'] = np.cos(2 * np.pi * X['day_val'] / 31)\n",
    "    X['sin_halfhour'] = np.sin(2 * np.pi * X['half_hour_interval'] / 48)\n",
    "    X['cos_halfhour'] = np.cos(2 * np.pi * X['half_hour_interval'] / 48)\n",
    "    X.drop(columns=['day_val', 'half_hour_interval'], inplace=True)\n",
    "\n",
    "    cat_cols = ['month_val', 'weekday_val'] + X.select_dtypes(include=['object']).columns.tolist()\n",
    "    cat_cols_exist = [col for col in cat_cols if col in X.columns]\n",
    "    X = pd.get_dummies(X, columns=cat_cols_exist, drop_first=False)\n",
    "\n",
    "    X = X.reindex(columns=selected_features_3_votes, fill_value=0)\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = prepare_xy_vote(df_train)\n",
    "X_valid, y_valid = prepare_xy_vote(df_valid)\n",
    "X_test, y_test = prepare_xy_vote(df_test)\n",
    "\n",
    "# model training (using GridSearchCV)\n",
    "params = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [4, 6],\n",
    "    'learning_rate': [0.05, 0.1]\n",
    "}\n",
    "model = XGBRegressor(random_state=42)\n",
    "grid = GridSearchCV(model, param_grid=params, cv=3, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# evaluation metrics\n",
    "def adjusted_r2(r2, n, k):\n",
    "    return 1 - (1 - r2) * ((n - 1) / (n - k - 1))\n",
    "\n",
    "def eval_regression(X, y, dataset_name):\n",
    "    y_pred = best_model.predict(X)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    adj_r2 = adjusted_r2(r2, len(y), X.shape[1])\n",
    "    print(f\"\\n📊 [{dataset_name} - 投票三票]\")\n",
    "    print(f\"MSE: {mse:.2f}\")\n",
    "    print(f\"Adjusted R²: {adj_r2:.4f}\")\n",
    "\n",
    "# evaluation\n",
    "# eval_regression(X_train, y_train, \"Train\")\n",
    "# eval_regression(X_valid, y_valid, \"Valid\")\n",
    "eval_regression(X_test, y_test, \"Test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ea7dde",
   "metadata": {},
   "source": [
    "Classification with All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aefb08",
   "metadata": {
    "id": "37aefb08",
    "outputId": "b00a26e3-12d0-495a-839d-5020fb0d2ef0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [20:05:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [20:05:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [20:05:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [20:05:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [20:05:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [20:05:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [20:05:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [20:05:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [20:05:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [20:05:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [20:05:34] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [20:05:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [20:05:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [20:05:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [20:05:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [20:05:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [20:06:03] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [20:06:05] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [20:06:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [20:06:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [20:06:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [20:06:24] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [20:06:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [20:06:30] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 [Test Set]\n",
      "Threshold: 0.2963\n",
      "Accuracy: 0.9475\n",
      "Precision: 0.9478\n",
      "Recall: 0.9996\n",
      "F1 Score: 0.9730\n",
      "ROC-AUC: 0.9227\n",
      "PRC-AUC: 0.9951\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score, precision_recall_curve\n",
    ")\n",
    "\n",
    "# data loading and preprocessing\n",
    "df1 = pd.read_csv(\"../區域資料_終版/中西_dataset.csv\")\n",
    "df2 = pd.read_csv(\"../區域資料_終版/安平_dataset.csv\")\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "df.loc[(df[\"ParkingSegmentID\"] == 1192) & (df[\"half_hour_interval\"] == 20) & (df[\"month_val\"] == 3) & (df[\"day_val\"] == 8), \"TotalSpaces\"] = 63\n",
    "df.drop(df[(df['ParkingSegmentID'] == 1131) & (df['year_val'] == 2025) & (df['month_val'] == 3) & (df['day_val'] == 27)].index, inplace=True)\n",
    "df.loc[df[\"avg_available_spots\"] > df[\"TotalSpaces\"], \"avg_available_spots\"] = df[\"TotalSpaces\"]\n",
    "\n",
    "# create classification labels: whether there is available space (>5%)\n",
    "df = df.dropna(subset=['avg_available_spots', 'TotalSpaces'])\n",
    "df['has_available'] = (df['avg_available_spots'] / df['TotalSpaces']) > 0.05\n",
    "df['has_available'] = df['has_available'].astype(int)\n",
    "\n",
    "# time and wind direction processing\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df['day_off'] = df['datetime'].dt.weekday.isin([5, 6]) | df['is_national_holiday']\n",
    "df['day_off'] = df['day_off'].astype(int)\n",
    "for height in [10, 80, 120]:\n",
    "    df[f'wind_direction_{height}m_sin'] = np.sin(np.deg2rad(df[f'wind_direction_{height}m']))\n",
    "    df[f'wind_direction_{height}m_cos'] = np.cos(np.deg2rad(df[f'wind_direction_{height}m']))\n",
    "df.drop(columns=[\n",
    "    \"wind_direction_10m\", \"wind_direction_80m\", \"wind_direction_120m\",\n",
    "    \"holiday_name\", \"is_national_holiday\"\n",
    "], inplace=True)\n",
    "\n",
    "# time split\n",
    "train_start = pd.to_datetime(\"2024-01-23\")\n",
    "train_end = pd.to_datetime(\"2025-01-22\")\n",
    "valid_start = pd.to_datetime(\"2025-01-23\")\n",
    "valid_end = pd.to_datetime(\"2025-03-22\")\n",
    "test_start = pd.to_datetime(\"2025-03-23\")\n",
    "test_end = pd.to_datetime(\"2025-05-09\")\n",
    "\n",
    "df_train = df[(df['datetime'] >= train_start) & (df['datetime'] <= train_end)].copy()\n",
    "df_valid = df[(df['datetime'] >= valid_start) & (df['datetime'] <= valid_end)].copy()\n",
    "df_test = df[(df['datetime'] >= test_start) & (df['datetime'] <= test_end)].copy()\n",
    "\n",
    "drop_cols = ['datetime', 'datetime_hour', 'time', 'date', 'ParkingSegmentID', 'avg_available_spots']\n",
    "for dataset in [df_train, df_valid, df_test]:\n",
    "    for col in drop_cols:\n",
    "        if col in dataset.columns:\n",
    "            dataset.drop(columns=col, inplace=True)\n",
    "\n",
    "# feature engineering\n",
    "def prepare_xy_class(df_part):\n",
    "    y = df_part['has_available']\n",
    "    X = df_part.drop(columns=['has_available'])\n",
    "\n",
    "    X['sin_day'] = np.sin(2 * np.pi * X['day_val'] / 31)\n",
    "    X['cos_day'] = np.cos(2 * np.pi * X['day_val'] / 31)\n",
    "    X['sin_halfhour'] = np.sin(2 * np.pi * X['half_hour_interval'] / 48)\n",
    "    X['cos_halfhour'] = np.cos(2 * np.pi * X['half_hour_interval'] / 48)\n",
    "    X.drop(columns=['day_val', 'half_hour_interval'], inplace=True)\n",
    "\n",
    "    cat_cols = ['month_val', 'weekday_val'] + X.select_dtypes(include=['object']).columns.tolist()\n",
    "    cat_cols_exist = [col for col in cat_cols if col in X.columns]\n",
    "    X = pd.get_dummies(X, columns=cat_cols_exist, drop_first=False)\n",
    "\n",
    "    X = X.reindex(columns=selected_features_4_votes, fill_value=0)\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = prepare_xy_class(df_train)\n",
    "X_valid, y_valid = prepare_xy_class(df_valid)\n",
    "X_test, y_test = prepare_xy_class(df_test)\n",
    "\n",
    "# grid search classifier\n",
    "params = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [4, 6],\n",
    "    'learning_rate': [0.05, 0.1]\n",
    "}\n",
    "clf = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "grid = GridSearchCV(clf, param_grid=params, cv=3, scoring='f1', n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train, y_train)\n",
    "best_clf = grid.best_estimator_\n",
    "\n",
    "# prediction probabilities and optimal threshold\n",
    "y_val_prob = best_clf.predict_proba(X_valid)[:, 1]\n",
    "precision, recall, thresholds = precision_recall_curve(y_valid, y_val_prob)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "# evaluation function\n",
    "def evaluate(y_true, y_prob, threshold, dataset_name):\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    roc = roc_auc_score(y_true, y_prob)\n",
    "    prc = average_precision_score(y_true, y_prob)\n",
    "    print(f\"\\n📊 [{dataset_name}]\")\n",
    "    print(f\"Threshold: {threshold:.4f}\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall: {rec:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC-AUC: {roc:.4f}\")\n",
    "    print(f\"PRC-AUC: {prc:.4f}\")\n",
    "\n",
    "# apply the optimal threshold to the test set\n",
    "y_test_prob = best_clf.predict_proba(X_test)[:, 1]\n",
    "evaluate(y_test, y_test_prob, best_threshold, \"Test Set\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501ea509",
   "metadata": {},
   "source": [
    "Elastic Net and Cost-Sensitive Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd64db3",
   "metadata": {
    "id": "0cd64db3",
    "outputId": "b96fe9aa-0979-469c-c658-dd48f8cf4a00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 [Test - XGBoost + Cost-sensitive]\n",
      "Accuracy: 0.9562\n",
      "Precision: 0.9562\n",
      "Recall: 1.0000\n",
      "F1: 0.9776\n",
      "ROC-AUC: 0.9415\n",
      "PRC-AUC: 0.9971\n",
      "Optimal Threshold: 0.01\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# preprocessing module\n",
    "def load_and_clean_data():\n",
    "    df1 = pd.read_csv(\"../區域資料_終版/中西_dataset.csv\")\n",
    "    df2 = pd.read_csv(\"../區域資料_終版/安平_dataset.csv\")\n",
    "    df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "    df.loc[(df[\"ParkingSegmentID\"] == 1192) & (df[\"half_hour_interval\"] == 20) &\n",
    "           (df[\"month_val\"] == 3) & (df[\"day_val\"] == 8), \"TotalSpaces\"] = 63\n",
    "    df.drop(df[(df['ParkingSegmentID'] == 1131) & (df['year_val'] == 2025) &\n",
    "               (df['month_val'] == 3) & (df['day_val'] == 27)].index, inplace=True)\n",
    "    df.loc[df[\"avg_available_spots\"] > df[\"TotalSpaces\"], \"avg_available_spots\"] = df[\"TotalSpaces\"]\n",
    "\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    df['day_off'] = df['datetime'].dt.weekday.isin([5, 6]) | df['is_national_holiday']\n",
    "    df['day_off'] = df['day_off'].astype(int)\n",
    "\n",
    "    for h in [10, 80, 120]:\n",
    "        df[f'wind_direction_{h}m_sin'] = np.sin(np.deg2rad(df[f'wind_direction_{h}m']))\n",
    "        df[f'wind_direction_{h}m_cos'] = np.cos(np.deg2rad(df[f'wind_direction_{h}m']))\n",
    "    df.drop(columns=[\n",
    "        \"wind_direction_10m\", \"wind_direction_80m\", \"wind_direction_120m\",\n",
    "        \"holiday_name\", \"is_national_holiday\"\n",
    "    ], inplace=True)\n",
    "\n",
    "    return df.dropna(subset=['avg_available_spots'])\n",
    "\n",
    "# time split\n",
    "def split_by_time(df):\n",
    "    train_start, train_end = pd.to_datetime(\"2024-01-23\"), pd.to_datetime(\"2025-01-22\")\n",
    "    valid_start, valid_end = pd.to_datetime(\"2025-01-23\"), pd.to_datetime(\"2025-03-22\")\n",
    "    test_start, test_end = pd.to_datetime(\"2025-03-23\"), pd.to_datetime(\"2025-05-09\")\n",
    "\n",
    "    df_train = df[(df['datetime'] >= train_start) & (df['datetime'] <= train_end)].copy()\n",
    "    df_valid = df[(df['datetime'] >= valid_start) & (df['datetime'] <= valid_end)].copy()\n",
    "    df_test = df[(df['datetime'] >= test_start) & (df['datetime'] <= test_end)].copy()\n",
    "\n",
    "    drop_cols = ['datetime', 'datetime_hour', 'time', 'date', 'ParkingSegmentID']\n",
    "    for d in [df_train, df_valid, df_test]:\n",
    "        for col in drop_cols:\n",
    "            if col in d.columns:\n",
    "                d.drop(columns=col, inplace=True)\n",
    "    return df_train, df_valid, df_test\n",
    "\n",
    "# feature processing module (for classification)\n",
    "def prepare_xy_for_classification(df_part, feature_list):\n",
    "    df_part['has_space'] = (df_part['avg_available_spots']/df_part['TotalSpaces'] > 0).astype(int)\n",
    "    y = df_part['has_space']\n",
    "    X = df_part.drop(columns=['avg_available_spots', 'has_space'])\n",
    "\n",
    "    # cyclical features\n",
    "    X['sin_day'] = np.sin(2 * np.pi * X['day_val'] / 31)\n",
    "    X['cos_day'] = np.cos(2 * np.pi * X['day_val'] / 31)\n",
    "    X['sin_halfhour'] = np.sin(2 * np.pi * X['half_hour_interval'] / 48)\n",
    "    X['cos_halfhour'] = np.cos(2 * np.pi * X['half_hour_interval'] / 48)\n",
    "    X.drop(columns=['day_val', 'half_hour_interval'], inplace=True)\n",
    "\n",
    "    cat_cols = ['month_val', 'weekday_val'] + X.select_dtypes(include=['object']).columns.tolist()\n",
    "    cat_cols_exist = [col for col in cat_cols if col in X.columns]\n",
    "    X = pd.get_dummies(X, columns=cat_cols_exist, drop_first=False)\n",
    "\n",
    "    X = X.reindex(columns=feature_list, fill_value=0)\n",
    "    return X, y\n",
    "\n",
    "# evaluation function\n",
    "def evaluate_classification(model, X, y, dataset_name):\n",
    "    y_prob = model.predict_proba(X)[:, 1]\n",
    "    thresholds = np.linspace(0, 1, 101)\n",
    "    best_f1, best_thresh = 0, 0.5\n",
    "\n",
    "    for t in thresholds:\n",
    "        y_pred = (y_prob >= t).astype(int)\n",
    "        f1 = f1_score(y, y_pred)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thresh = f1, t\n",
    "\n",
    "    y_pred = (y_prob >= best_thresh).astype(int)\n",
    "\n",
    "    print(f\"\\n📊 [{dataset_name} - XGBoost + Cost-sensitive]\")\n",
    "    print(f\"Accuracy: {accuracy_score(y, y_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y, y_pred):.4f}\")\n",
    "    print(f\"Recall: {recall_score(y, y_pred):.4f}\")\n",
    "    print(f\"F1: {f1_score(y, y_pred):.4f}\")\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y, y_prob):.4f}\")\n",
    "    print(f\"PRC-AUC: {average_precision_score(y, y_prob):.4f}\")\n",
    "    print(f\"Optimal Threshold: {best_thresh:.2f}\")\n",
    "\n",
    "df = load_and_clean_data()\n",
    "df_train, df_valid, df_test = split_by_time(df)\n",
    "\n",
    "# Elastic Net feature list\n",
    "selected_features_elastic_net = [\n",
    "\"laterHourFee\",\"firstHourFee\",\"day_off\",\"district\",\n",
    "\"half_hour_cos\",\"lat\",\"year_val\",\"month_val_2\",\"TotalSpaces\",\n",
    "\"month_val_12\",\"lon\",\"terrestrial_radiation_instant\",\"dew_point_2m\",\n",
    "\"weekday_num_2\",\"day_sin\",\"weekday_num_1\",\"half_hour_sin\",\n",
    "\"month_val_5\",\"cloud_cover_mid\",\"weekday_num_0\",\"pressure_msl\",\"vapour_pressure_deficit\",\n",
    "\"month_val_11\",\"wind_speed_10m\",'cloud_cover','precipitation','surface_pressure']  #Elastic Net 特徵\n",
    "\n",
    "X_train, y_train = prepare_xy_for_classification(df_train, selected_features_elastic_net)\n",
    "X_valid, y_valid = prepare_xy_for_classification(df_valid, selected_features_elastic_net)\n",
    "X_test, y_test = prepare_xy_for_classification(df_test, selected_features_elastic_net)\n",
    "\n",
    "# class imbalance weights\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "# training XGBoost classification model\n",
    "clf = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    n_estimators=200,\n",
    "    random_state=42\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# model evaluation\n",
    "# evaluate_classification(clf, X_train, y_train, \"Train\")\n",
    "# evaluate_classification(clf, X_valid, y_valid, \"Valid\")\n",
    "evaluate_classification(clf, X_test, y_test, \"Test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb79a19",
   "metadata": {},
   "source": [
    "Classification with Features Selected by Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e481b4f6",
   "metadata": {
    "id": "e481b4f6",
    "outputId": "36c57fe8-9443-44a4-e004-57b40f5ec480"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:59:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:59:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:59:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:59:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:59:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:59:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:59:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:59:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:59:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:59:56] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [08:59:56] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [09:00:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [09:00:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [09:00:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [09:00:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [09:00:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [09:00:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [09:00:29] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [09:00:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [09:00:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [09:00:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [09:00:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [09:00:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [09:01:05] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 [Test Set]\n",
      "Threshold: 0.4774\n",
      "Accuracy: 0.9465\n",
      "Precision: 0.9553\n",
      "Recall: 0.9899\n",
      "F1 Score: 0.9723\n",
      "ROC-AUC: 0.9274\n",
      "PRC-AUC: 0.9954\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score, precision_recall_curve\n",
    ")\n",
    "\n",
    "# Elastic Net feature list\n",
    "selected_features_4_votes = [\n",
    "    'laterHourFee', 'firstHourFee', 'day_off', 'district', 'half_hour_cos',\n",
    "    'lat', 'year_val', 'month_val_2', 'TotalSpaces', 'month_val_12', 'lon',\n",
    "    'terrestrial_radiation_instant', 'dew_point_2m', 'weekday_num_2', 'day_sin',\n",
    "    'weekday_num_1', 'half_hour_sin', 'month_val_5', 'cloud_cover_mid',\n",
    "    'weekday_num_0', 'pressure_msl', 'vapour_pressure_deficit',\n",
    "    'wind_speed_10m', 'cloud_cover', 'precipitation', 'surface_pressure'\n",
    "]\n",
    "\n",
    "# data loading and preprocessing\n",
    "df1 = pd.read_csv(\"../區域資料_終版/中西_dataset.csv\")\n",
    "df2 = pd.read_csv(\"../區域資料_終版/安平_dataset.csv\")\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "df.loc[(df[\"ParkingSegmentID\"] == 1192) & (df[\"half_hour_interval\"] == 20) & (df[\"month_val\"] == 3) & (df[\"day_val\"] == 8), \"TotalSpaces\"] = 63\n",
    "df.drop(df[(df['ParkingSegmentID'] == 1131) & (df['year_val'] == 2025) & (df['month_val'] == 3) & (df['day_val'] == 27)].index, inplace=True)\n",
    "df.loc[df[\"avg_available_spots\"] > df[\"TotalSpaces\"], \"avg_available_spots\"] = df[\"TotalSpaces\"]\n",
    "\n",
    "# create classification labels: whether there is available space (>5%)\n",
    "df = df.dropna(subset=['avg_available_spots', 'TotalSpaces'])\n",
    "df['has_available'] = (df['avg_available_spots'] / df['TotalSpaces']) > 0.05\n",
    "df['has_available'] = df['has_available'].astype(int)\n",
    "\n",
    "# time and wind direction processing\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df['day_off'] = df['datetime'].dt.weekday.isin([5, 6]) | df['is_national_holiday']\n",
    "df['day_off'] = df['day_off'].astype(int)\n",
    "for height in [10, 80, 120]:\n",
    "    df[f'wind_direction_{height}m_sin'] = np.sin(np.deg2rad(df[f'wind_direction_{height}m']))\n",
    "    df[f'wind_direction_{height}m_cos'] = np.cos(np.deg2rad(df[f'wind_direction_{height}m']))\n",
    "df.drop(columns=[\n",
    "    \"wind_direction_10m\", \"wind_direction_80m\", \"wind_direction_120m\",\n",
    "    \"holiday_name\", \"is_national_holiday\"\n",
    "], inplace=True)\n",
    "\n",
    "# time features for half_hour_cos and half_hour_sin, day_sin\n",
    "# ensure day_val and half_hour_interval are available before dropping\n",
    "df['half_hour_cos'] = np.cos(2 * np.pi * df['half_hour_interval'] / 48)\n",
    "df['half_hour_sin'] = np.sin(2 * np.pi * df['half_hour_interval'] / 48)\n",
    "df['day_sin'] = np.sin(2 * np.pi * df['day_val'] / 31) # This is assumed to be in selected_features.\n",
    "# if day_val is not in selected_features, this might not be needed.\n",
    "# for now, keeping it as it's required for day_sin.\n",
    "\n",
    "# time split\n",
    "train_start = pd.to_datetime(\"2024-01-23\")\n",
    "train_end = pd.to_datetime(\"2025-01-22\")\n",
    "valid_start = pd.to_datetime(\"2025-01-23\")\n",
    "valid_end = pd.to_datetime(\"2025-03-22\")\n",
    "test_start = pd.to_datetime(\"2025-03-23\")\n",
    "test_end = pd.to_datetime(\"2025-05-09\")\n",
    "\n",
    "df_train = df[(df['datetime'] >= train_start) & (df['datetime'] <= train_end)].copy()\n",
    "df_valid = df[(df['datetime'] >= valid_start) & (df['datetime'] <= valid_end)].copy()\n",
    "df_test = df[(df['datetime'] >= test_start) & (df['datetime'] <= test_end)].copy()\n",
    "\n",
    "# these columns should be dropped before feature selection if they are not features\n",
    "drop_cols_before_feature_selection = ['datetime', 'datetime_hour', 'time', 'date', 'ParkingSegmentID', 'avg_available_spots']\n",
    "for dataset in [df_train, df_valid, df_test]:\n",
    "    for col in drop_cols_before_feature_selection:\n",
    "        if col in dataset.columns:\n",
    "            dataset.drop(columns=col, inplace=True)\n",
    "\n",
    "# feature engineering\n",
    "def prepare_xy_class(df_part):\n",
    "    y = df_part['has_available']\n",
    "    # Drop has_available from X\n",
    "    X = df_part.drop(columns=['has_available'])\n",
    "\n",
    "    # handle categorical columns by one-hot encoding if they are in the selected features\n",
    "    # district and month_val and weekday_num related features from the list suggest one-hot encoding might be needed\n",
    "    # first, identify the base categorical columns that might need one-hot encoding\n",
    "    base_cat_cols = ['district', 'month_val', 'weekday_num'] # Assuming these exist as base columns before dummy creation\n",
    "\n",
    "    # check which base categorical columns exist in the current DataFrame\n",
    "    actual_base_cat_cols = [col for col in base_cat_cols if col in X.columns]\n",
    "\n",
    "    # perform one-hot encoding\n",
    "    if actual_base_cat_cols:\n",
    "        X = pd.get_dummies(X, columns=actual_base_cat_cols, drop_first=False) # drop_first=False to keep all dummy variables as specified in selected_features_4_votes (e.g., month_val_2, month_val_12, weekday_num_0, weekday_num_1, weekday_num_2)\n",
    "\n",
    "    # reindex X to only include the selected features, filling missing with 0\n",
    "    # this is crucial for matching the exact feature set\n",
    "    X = X.reindex(columns=selected_features_4_votes, fill_value=0)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = prepare_xy_class(df_train)\n",
    "X_valid, y_valid = prepare_xy_class(df_valid)\n",
    "X_test, y_test = prepare_xy_class(df_test)\n",
    "\n",
    "# grid search classifier\n",
    "params = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [4, 6],\n",
    "    'learning_rate': [0.05, 0.1]\n",
    "}\n",
    "clf = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "grid = GridSearchCV(clf, param_grid=params, cv=3, scoring='f1', n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train, y_train)\n",
    "best_clf = grid.best_estimator_\n",
    "\n",
    "# prediction probabilities and optimal threshold\n",
    "y_val_prob = best_clf.predict_proba(X_valid)[:, 1]\n",
    "precision, recall, thresholds = precision_recall_curve(y_valid, y_val_prob)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "# evalution function\n",
    "def evaluate(y_true, y_prob, threshold, dataset_name):\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    roc = roc_auc_score(y_true, y_prob)\n",
    "    prc = average_precision_score(y_true, y_prob)\n",
    "    print(f\"\\n📊 [{dataset_name}]\")\n",
    "    print(f\"Threshold: {threshold:.4f}\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall: {rec:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC-AUC: {roc:.4f}\")\n",
    "    print(f\"PRC-AUC: {prc:.4f}\")\n",
    "\n",
    "# apply the optimal threshold to the test set\n",
    "y_test_prob = best_clf.predict_proba(X_test)[:, 1]\n",
    "evaluate(y_test, y_test_prob, best_threshold, \"Test Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3dc0a4",
   "metadata": {},
   "source": [
    "Classification with Features Selected by Elastic Net and Perform Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106619b8",
   "metadata": {
    "id": "106619b8",
    "outputId": "8bd41aa4-e3c3-499f-ace2-ea08f645cfa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using common Elastic Net features: ['laterHourFee', 'firstHourFee', 'day_off', 'district', 'half_hour_cos', 'lat', 'year_val', 'month_val_2', 'TotalSpaces', 'month_val_12', 'lon', 'terrestrial_radiation_instant', 'dew_point_2m', 'weekday_num_2', 'day_sin', 'weekday_num_1', 'half_hour_sin', 'month_val_5', 'cloud_cover_mid', 'weekday_num_0', 'pressure_msl', 'vapour_pressure_deficit', 'month_val_11', 'wind_speed_10m', 'cloud_cover', 'precipitation', 'surface_pressure']\n",
      "Warning: 'datetime' not found in df_test_out.csv. 'day_off' calculation and other time-based operations might be affected for df_test.\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "\n",
      "📊 [Test - Elastic Net + SMOTE + XGBoost]\n",
      "Accuracy: 0.9072\n",
      "Precision: 0.9742\n",
      "Recall: 0.9265\n",
      "F1-score: 0.9498\n",
      "ROC-AUC: 0.9015\n",
      "PRC-AUC: 0.9937\n",
      "Best Threshold (by F1): 0.0546\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, precision_recall_curve\n",
    "from xgboost import XGBClassifier \n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# initial Elastic Net feature list\n",
    "initial_elastic_net_features = [\n",
    "    \"laterHourFee\", \"firstHourFee\", \"day_off\",\n",
    "    \"district\", \"half_hour_cos\", \"lat\",\n",
    "    \"year_val\", \"month_val_2\", \"TotalSpaces\",\n",
    "    \"month_val_12\", \"lon\", \"terrestrial_radiation_instant\",\n",
    "    \"dew_point_2m\", \"weekday_num_2\", \"day_sin\",\n",
    "    \"weekday_num_1\", \"half_hour_sin\", \"month_val_5\",\n",
    "    \"cloud_cover_mid\", \"weekday_num_0\", \"pressure_msl\",\n",
    "    \"vapour_pressure_deficit\", \"month_val_11\", \"wind_speed_10m\",\n",
    "    \"cloud_cover\", \"precipitation\", \"surface_pressure\"\n",
    "]\n",
    "\n",
    "# load data\n",
    "# use smote_data_raw.csv as the source for training and validation data\n",
    "df = pd.read_csv(\"smote_data_raw.csv\")\n",
    "# use df_test_out.csv as the source for test data\n",
    "df_test = pd.read_csv(\"df_test_out.csv\")\n",
    "\n",
    "# identify the common Elastic Net selected features from both datasets\n",
    "common_features = [\n",
    "    col for col in initial_elastic_net_features\n",
    "    if col in df.columns and col in df_test.columns\n",
    "]\n",
    "elastic_net_features = common_features\n",
    "\n",
    "print(f\"Using common Elastic Net features: {elastic_net_features}\")\n",
    "if not elastic_net_features:\n",
    "    print(\"Warning: No common features found between initial_elastic_net_features and loaded dataframes. Check your feature list and CSV columns.\")\n",
    "\n",
    "# ensure that elastic_net_features does not include the target variable is_full\n",
    "if 'is_full' in elastic_net_features:\n",
    "    elastic_net_features.remove('is_full')\n",
    "\n",
    "# correct errors and preprocess (for smote_data_raw.csv)\n",
    "\n",
    "# use 20% as the validation set\n",
    "df_train, df_valid = train_test_split(df, test_size=0.2, random_state=42) \n",
    "\n",
    "# process test data df_test (df_test_out.csv)\n",
    "# df_test_out.csv contains the datetime column, so keep the related processing steps\n",
    "if 'datetime' in df_test.columns:\n",
    "    df_test['datetime'] = pd.to_datetime(df_test['datetime'])\n",
    "    # Recalculate day_off because df_test_out.csv includes is_national_holiday\n",
    "    if 'is_national_holiday' in df_test.columns:\n",
    "        df_test['day_off'] = df_test['datetime'].dt.weekday.isin([5, 6]) | df_test['is_national_holiday']\n",
    "    else:\n",
    "        print(\"Warning: 'is_national_holiday' not found in df_test. 'day_off' will only consider weekends for df_test.\")\n",
    "        df_test['day_off'] = df_test['datetime'].dt.weekday.isin([5, 6])\n",
    "    df_test['day_off'] = df_test['day_off'].astype(int)\n",
    "else:\n",
    "    print(\"Warning: 'datetime' not found in df_test_out.csv. 'day_off' calculation and other time-based operations might be affected for df_test.\")\n",
    "    if 'day_off' not in df_test.columns:\n",
    "        print(\"Error: 'day_off' not found and cannot be calculated for df_test without 'datetime'.\")\n",
    "\n",
    "\n",
    "# clipping of avg_available_spots \n",
    "if 'avg_available_spots' in df_test.columns and 'TotalSpaces' in df_test.columns:\n",
    "    df_test['avg_available_spots'] = df_test['avg_available_spots'].clip(upper=df_test['TotalSpaces'])\n",
    "else:\n",
    "    print(\"Warning: 'avg_available_spots' or 'TotalSpaces' not found in df_test_out.csv. Skipping clipping for df_test.\")\n",
    "\n",
    "# wind_direction_Xm_sin/cos are already in df_test_out.csv, no need to recalculate\n",
    "# dynamically remove columns from df_test that are not in elastic_net_features or required for target/time processing\n",
    "all_df_test_cols = df_test.columns.tolist()\n",
    "cols_to_keep_for_df_test = set(elastic_net_features + ['datetime', 'avg_available_spots', 'TotalSpaces', 'is_national_holiday']) # 確保 is_national_holiday 也被保留用於 day_off 計算\n",
    "\n",
    "columns_to_drop_df_test = [\n",
    "    col for col in all_df_test_cols\n",
    "    if col not in cols_to_keep_for_df_test\n",
    "]\n",
    "for col in columns_to_drop_df_test:\n",
    "    if col in df_test.columns:\n",
    "        df_test.drop(columns=[col], inplace=True)\n",
    "\n",
    "\n",
    "# create classification labels (predict whether there is available parking)\n",
    "# use is_full as the target variable for the training and validation sets\n",
    "for data in [df_train, df_valid]:\n",
    "    if 'is_full' in data.columns:\n",
    "        # assuming is_full = 1 means full and is_full = 0 means available spaces\n",
    "        # the target is \"has space,\" so when is_full = 0, set has_space = 1; when is_full = 1, set has_space = 0\n",
    "        data['has_space'] = (1 - data['is_full']).astype(int)\n",
    "    else:\n",
    "        print(\"Error: 'is_full' not found in smote_data_raw.csv. Cannot create 'has_space' target for train/valid.\")\n",
    "\n",
    "# the test set uses avg_available_spots and TotalSpaces to calculate has_space\n",
    "if 'avg_available_spots' in df_test.columns and 'TotalSpaces' in df_test.columns:\n",
    "    df_test['has_space'] = (df_test['avg_available_spots'] / df_test['TotalSpaces'].replace(0, np.nan)) > 0.05\n",
    "    df_test['has_space'] = df_test['has_space'].astype(int)\n",
    "else:\n",
    "    print(\"Error: Cannot create 'has_space' target for test. 'avg_available_spots' or 'TotalSpaces' missing in df_test_out.csv.\")\n",
    "\n",
    "# features and target variable\n",
    "# ensure all features exist in the dataset\n",
    "# here's a check that issues a warning if some Elastic Net selected features are missing in the dataset\n",
    "\n",
    "# check the training set features\n",
    "missing_train_features = [col for col in elastic_net_features if col not in df_train.columns]\n",
    "if missing_train_features:\n",
    "    print(f\"Error: Missing features in X_train: {missing_train_features}\")\n",
    "    print(\"Please check if 'smote_data_raw.csv' contains these columns or if preprocessing steps are correct.\")\n",
    "\n",
    "# check the test set features\n",
    "missing_test_features = [col for col in elastic_net_features if col not in df_test.columns]\n",
    "if missing_test_features:\n",
    "    print(f\"Error: Missing features in X_test: {missing_test_features}\")\n",
    "    print(\"Please check if 'df_test_out.csv' contains these columns or if preprocessing steps are correct.\")\n",
    "\n",
    "\n",
    "X_train = df_train[elastic_net_features]\n",
    "X_valid = df_valid[elastic_net_features]\n",
    "X_test = df_test[elastic_net_features]\n",
    "\n",
    "# use the transformed has_space from is_full as the target for the training and validation sets\n",
    "y_train = df_train['has_space']\n",
    "y_valid = df_valid['has_space']\n",
    "# use the computed has_space as the target for the test set\n",
    "y_test = df_test['has_space']\n",
    "\n",
    "# standardization + SMOTE\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_smote, y_train_smote = SMOTE(random_state=42).fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# model training\n",
    "params = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [4, 6],\n",
    "    'learning_rate': [0.05, 0.1]\n",
    "}\n",
    "\n",
    "xgb_clf = XGBClassifier(random_state=42, eval_metric='logloss', objective='binary:logistic')\n",
    "grid = GridSearchCV(xgb_clf, param_grid=params, scoring='f1', cv=3, n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train_smote, y_train_smote)\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# evaluation function\n",
    "def evaluate_model(X, y, dataset_name):\n",
    "    # check if X is empty to avoid making predictions on empty data\n",
    "    if X.shape[0] == 0:\n",
    "        print(f\"\\n📊 [{dataset_name} - Elastic Net + SMOTE + XGBoost]\")\n",
    "        print(f\"No data to evaluate for {dataset_name}.\")\n",
    "        return\n",
    "\n",
    "    proba = best_model.predict_proba(X)[:, 1]\n",
    "    preds = (proba >= 0.5).astype(int)\n",
    "\n",
    "    acc = accuracy_score(y, preds)\n",
    "    prec = precision_score(y, preds)\n",
    "    rec = recall_score(y, preds)\n",
    "    f1 = f1_score(y, preds)\n",
    "    roc_auc = roc_auc_score(y, proba)\n",
    "    prc_auc = average_precision_score(y, proba)\n",
    "\n",
    "    # search for the best threshold\n",
    "    # ensure that y and proba are not empty and contain at least two classes before computing the precision_recall_curve\n",
    "    if len(np.unique(y)) > 1 and len(y) > 0:\n",
    "        precision, recall, thresholds = precision_recall_curve(y, proba)\n",
    "        # to avoid division by zero, add a small epsilon\n",
    "        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "        best_idx = np.argmax(f1_scores)\n",
    "        best_threshold = thresholds[best_idx]\n",
    "    else:\n",
    "        best_threshold = \"N/A (Insufficient data for PRC)\"\n",
    "\n",
    "\n",
    "    print(f\"\\n📊 [{dataset_name} - Elastic Net + SMOTE + XGBoost]\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall: {rec:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "    print(f\"PRC-AUC: {prc_auc:.4f}\")\n",
    "    print(f\"Best Threshold (by F1): {best_threshold:.4f}\")\n",
    "\n",
    "# evaluation\n",
    "# evaluate_model(X_train_scaled, y_train, \"Train\") \n",
    "# evaluate_model(X_valid_scaled, y_valid, \"Valid\") \n",
    "evaluate_model(X_test_scaled, y_test, \"Test\") \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
